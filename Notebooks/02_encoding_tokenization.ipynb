{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM5DC6tQlqufpobNl1Hc9YE"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import and Read the data"
      ],
      "metadata": {
        "id": "T_-GaHlNi3sZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kPDjZ9zhmM1e"
      },
      "outputs": [],
      "source": [
        "# read the input data file\n",
        "with open('raw-text-data.txt', 'r', encoding='utf-8') as f:\n",
        "    text_data = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# length of the data in chars\n",
        "print(\"length of the data in chars:\", len(text_data))\n",
        "\n",
        "# let's look at the first 100 characters\n",
        "print(text_data[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKyNzeQ5jlii",
        "outputId": "ac7c93f9-c6e9-4a2e-8c4a-2ad26ea55804"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of the data in chars: 15795757\n",
            "This page allows users to search multiple sources for a book given a 10- or 13-digit International S\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EmF3QQUX1kwK"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoders and Tokenizers"
      ],
      "metadata": {
        "id": "kmKm2TQG6w4V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "some popular tokenizers to look out for:\n",
        "- [sentencepiece](https://github.com/google/sentencepiece) (google)\n",
        "- [tiktoken](https://github.com/openai/tiktoken) (openai)\n",
        "- [word2vec](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/word2vec.ipynb) (google tf)\n",
        "- [code2vec](https://github.com/tech-srl/code2vec) (code2vec.org)\n",
        "- [GloVe](https://nlp.stanford.edu/projects/glove/) (nlp stanford)\n",
        "- [spaCy](https://spacy.io/usage/linguistic-features#vectors-similarity) (explosion.ai)\n",
        "\n",
        "we can look up hugging face, [chapter 6](https://huggingface.co/course/chapter6/1) for more tokenizers and [tokenizer_summary](https://huggingface.co/docs/transformers/tokenizer_summary) from their transformers page. some of the useful encoders are as below:\n",
        "- Byte Pair Encoding (BPE)\n",
        "- WordPiece tokenization\n",
        "- Unigram tokenization\n"
      ],
      "metadata": {
        "id": "GQD991eSqF5V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "for tokenizing and embedding code (say for example, C++), we can use:\n",
        "- [Subword tokenizers](https://www.tensorflow.org/text/guide/subwords_tokenizer)\n",
        "- Syntax-aware Tokenization\n",
        "- Code-specific Tokenization\n",
        "- Clang\n",
        "- TreeSitter\n",
        "- CodeBERT\n",
        "- Code2Vec\n",
        "- Graph-based Neural Networks"
      ],
      "metadata": {
        "id": "UEfHnQ_L06f7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let's build a basic, simple char embedding to int to start with."
      ],
      "metadata": {
        "id": "lgQGNRB21nQR"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's look at the unique chars in the text data\n",
        "char_vocab = sorted(list(set(text_data)))\n",
        "vocab_size = len(char_vocab)\n",
        "print(\"char vocabulary:\", ''.join(char_vocab))\n",
        "print(\"vocabulary size:\", vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lvED03p-jmQV",
        "outputId": "5d451b67-ee1a-42e0-dfce-54d5a2846993"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char vocabulary: \t\n",
            " !\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyz{|}~ ¡¢£¥§¬­®¯°±²³´µ·º»¼½ÀÁÄÅÆÇÉÎÑÓÕÖ×ØÜÞßàáâãäåæçèéêëìíîïðñòóôõöøùúûüýþĀāăąćċČčĐđēęěĝğġħĩĪīİıľŁłńņňŋŌōőŒœřŚśŞşŠšţũūŵŷźŻżŽžƒơưƿǂǎǐǒǔǣǥǧǫǵǷȘșȚțȟȳɐɑɒɔɕəɛɜɡɣɤɦɪɬɯɲɴɾʁʂʃʊʌʎʏʐʑʒʔʕʰʷʻʼʾʿˈˌːˤ̸̩̪̯̰̲̀́̃̄̊̌ͤͭ̚͡ΆΐΑΓΔΕΘΙΚΛΜΟΠΣΤΦΧΩάέήίαβγδεζηθικλμνξοπρςστυφχψωϊόύώϕϵАБГДКНОПРСТУФХабвгдезийклмнопрстуфхцчъьяїјҳְֲִֵֶַָֹֻּׁׂ֣אבגדהוחטיךכלםמןנסעפצקרשת׳،ءآأإئابةتثجحخدذرزسشصضطظعغفقكلمنهوىيَُِّْٰٱچکیचतदनबमरवशहाीे्ংকগজঞঠতথদনবরলািীু্ംമയലളാกขฃคฅงจฉญณดตถทธนบปผฝพฟภมยรลวษสหอฮะัาำิีืุูเแโใไๆ็่้ကငစညတထဒနပမယသာိီုေံး္်ြᚼᛏᛒḇḍḏḗḤḥḱḳṁṃṅṆṇṓṛṟṣṭṯṵẊẋẓẚạảấầẩậắếềểễệịọốồổộớờởỦủứữỳἀἁἄἈἐἑἔἕἙἠἡἦἰἴἶἷἸὁὅὐὑὖὤὰὲὴὶὸᾱῆῐῖῡῦῶ  ​‌‎‐–—―‖‘’“”†‡•… ‰′″‿⁄⁠⁡€₹ℏℓℝ™ℵℶ←→↔↦⇒∀∂∃∅∆∇∈∉∋∑−∗∘∙√∝∞∣∥∧∨∫∴∼≈≠≡≤≥≪≫⊂⊆⊚⊢⊨⋃⋅⋆⋯⌈⌉⌘▼★♭♮♯✓⟨⟩かじんアイガザスズベボマムユラリルンーㅇ一下业中主义之乐产人仁代企伙会但体佛促俗儒八公共兴农利华合名启命和嘉国國圳墨士天夫嫻子字孝学學宗定家密富實寶封小左市帝平康建当彬德忠成戰抓投教文新日春時會期本果桃權正武民汇法津深源漢爲王现現理生產由當百皇相眠睿矿社神禮秋简管約組經繁网義翼能自致芒花芳莊華融视訓詁語諧諸谐贾资跃運道鍾鑫银陰陽革音韻频體국리아어역카프한ﬁﺟ﻿（），𐤁𐤋𐤍𓂋𓈉𓈖𓏠\n",
            "vocabulary size: 953\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 802 chars! \n",
        "# I know we should do data cleaning, unwanted char/word removal, etc etc...\n",
        "# but let's just go with it and see what happens..."
      ],
      "metadata": {
        "id": "1w5g0R2KprSy"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a simple char to int tokenization strategy (encoding & decoding)\n",
        "\n",
        "# mapping between chars and ints vased on data vocab\n",
        "c2i = { ch:i for i,ch in enumerate(char_vocab) }\n",
        "i2c = { i:ch for i,ch in enumerate(char_vocab) }\n",
        "\n",
        "# encode: text string --> 1D num vector\n",
        "encode = lambda txt_str: [c2i[ch] for ch in txt_str]\n",
        "\n",
        "# decode: encoded 1D num vector --> text string\n",
        "decode = lambda num_vect: ''.join([i2c[i] for i in num_vect])"
      ],
      "metadata": {
        "id": "zft3QEX0lFVz"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's test our encoder and decoder\n",
        "print(encode(\"hello LLM o/\"))\n",
        "print(decode([74, 71, 78, 78, 81, 2, 46, 46, 47, 2, 81, 17]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8rqOtBsm4T8",
        "outputId": "f66cb590-466e-44f7-b7b5-8f82938b5903"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[74, 71, 78, 78, 81, 2, 46, 46, 47, 2, 81, 17]\n",
            "hello LLM o/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's see how it looks with first 100 chars from our raw text data\n",
        "print(text_data[:100])\n",
        "print(encode(text_data[:100]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FujrZtLypfVS",
        "outputId": "3195aaa5-3cdc-4e01-a371-17b4ecb4c359"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This page allows users to search multiple sources for a book given a 10- or 13-digit International S\n",
            "[54, 74, 75, 85, 2, 82, 67, 73, 71, 2, 67, 78, 78, 81, 89, 85, 2, 87, 85, 71, 84, 85, 2, 86, 81, 2, 85, 71, 67, 84, 69, 74, 2, 79, 87, 78, 86, 75, 82, 78, 71, 2, 85, 81, 87, 84, 69, 71, 85, 2, 72, 81, 84, 2, 67, 2, 68, 81, 81, 77, 2, 73, 75, 88, 71, 80, 2, 67, 2, 19, 18, 15, 2, 81, 84, 2, 19, 21, 15, 70, 75, 73, 75, 86, 2, 43, 80, 86, 71, 84, 80, 67, 86, 75, 81, 80, 67, 78, 2, 53]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# so our basic encoding works!"
      ],
      "metadata": {
        "id": "T1jOGvz4pmDr"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tt4oYmCF41SJ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using the Encoder/Tokenizer for Input Embedding"
      ],
      "metadata": {
        "id": "nrsvq94w64-t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let's now encode the entire text dataset and store it into torch.Tensor\n",
        "# (blindly following Karpathy here), using PyTorch (https://pytorch.org)\n",
        "# could have used TF, or other techniques, but will see in future\n",
        "\n",
        "import torch"
      ],
      "metadata": {
        "id": "IOFqQ1rYq6ZP"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use our encoder and wrap it into PyTorch tensors\n",
        "data = torch.tensor(encode(text_data), dtype=torch.long)\n",
        "\n",
        "# let's take a look at shape and dtype\n",
        "print(data.shape, data.dtype)\n",
        "\n",
        "# and the first 100 chars from earlier\n",
        "print(data[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8GG_03XHq6ch",
        "outputId": "1ee84ff3-a4b4-49f4-9632-18514b502757"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([15795757]) torch.int64\n",
            "tensor([54, 74, 75, 85,  2, 82, 67, 73, 71,  2, 67, 78, 78, 81, 89, 85,  2, 87,\n",
            "        85, 71, 84, 85,  2, 86, 81,  2, 85, 71, 67, 84, 69, 74,  2, 79, 87, 78,\n",
            "        86, 75, 82, 78, 71,  2, 85, 81, 87, 84, 69, 71, 85,  2, 72, 81, 84,  2,\n",
            "        67,  2, 68, 81, 81, 77,  2, 73, 75, 88, 71, 80,  2, 67,  2, 19, 18, 15,\n",
            "         2, 81, 84,  2, 19, 21, 15, 70, 75, 73, 75, 86,  2, 43, 80, 86, 71, 84,\n",
            "        80, 67, 86, 75, 81, 80, 67, 78,  2, 53])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ef-kZxid5Mbo"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cFn4kLiT5MeF"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-qxITsIe5Mm2"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References"
      ],
      "metadata": {
        "id": "gj9uXSk77LBU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reading Materials and References for future:\n",
        "- https://towardsdatascience.com/word-embeddings-with-code2vec-glove-and-spacy-5b26420bf632\n",
        "- https://huggingface.co/course/chapter6/8\n",
        "- https://neptune.ai/blog/tokenization-in-nlp\n",
        "- https://research.aimultiple.com/large-language-model-training/\n",
        "- https://towardsdatascience.com/word-embeddings-in-2020-review-with-code-examples-11eb39a1ee6d\n"
      ],
      "metadata": {
        "id": "mxaoRwUfq61w"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1hSdp2MurEa2"
      },
      "execution_count": 11,
      "outputs": []
    }
  ]
}