{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM5DC6tQlqufpobNl1Hc9YE"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import and Read the data"
      ],
      "metadata": {
        "id": "T_-GaHlNi3sZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kPDjZ9zhmM1e"
      },
      "outputs": [],
      "source": [
        "# read the input data file\n",
        "with open('raw-text-data.txt', 'r', encoding='utf-8') as f:\n",
        "    text_data = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# length of the data in chars\n",
        "print(\"length of the data in chars:\", len(text_data))\n",
        "\n",
        "# let's look at the first 100 characters\n",
        "print(text_data[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKyNzeQ5jlii",
        "outputId": "ac7c93f9-c6e9-4a2e-8c4a-2ad26ea55804"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of the data in chars: 15795757\n",
            "This page allows users to search multiple sources for a book given a 10- or 13-digit International S\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EmF3QQUX1kwK"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoders and Tokenizers"
      ],
      "metadata": {
        "id": "kmKm2TQG6w4V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "some popular tokenizers to look out for:\n",
        "- [sentencepiece](https://github.com/google/sentencepiece) (google)\n",
        "- [tiktoken](https://github.com/openai/tiktoken) (openai)\n",
        "- [word2vec](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/word2vec.ipynb) (google tf)\n",
        "- [code2vec](https://github.com/tech-srl/code2vec) (code2vec.org)\n",
        "- [GloVe](https://nlp.stanford.edu/projects/glove/) (nlp stanford)\n",
        "- [spaCy](https://spacy.io/usage/linguistic-features#vectors-similarity) (explosion.ai)\n",
        "\n",
        "we can look up hugging face, [chapter 6](https://huggingface.co/course/chapter6/1) for more tokenizers and [tokenizer_summary](https://huggingface.co/docs/transformers/tokenizer_summary) from their transformers page. some of the useful encoders are as below:\n",
        "- Byte Pair Encoding (BPE)\n",
        "- WordPiece tokenization\n",
        "- Unigram tokenization\n"
      ],
      "metadata": {
        "id": "GQD991eSqF5V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "for tokenizing and embedding code (say for example, C++), we can use:\n",
        "- [Subword tokenizers](https://www.tensorflow.org/text/guide/subwords_tokenizer)\n",
        "- Syntax-aware Tokenization\n",
        "- Code-specific Tokenization\n",
        "- Clang\n",
        "- TreeSitter\n",
        "- CodeBERT\n",
        "- Code2Vec\n",
        "- Graph-based Neural Networks"
      ],
      "metadata": {
        "id": "UEfHnQ_L06f7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let's build a basic, simple char embedding to int to start with."
      ],
      "metadata": {
        "id": "lgQGNRB21nQR"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's look at the unique chars in the text data\n",
        "char_vocab = sorted(list(set(text_data)))\n",
        "vocab_size = len(char_vocab)\n",
        "print(\"char vocabulary:\", ''.join(char_vocab))\n",
        "print(\"vocabulary size:\", vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lvED03p-jmQV",
        "outputId": "5d451b67-ee1a-42e0-dfce-54d5a2846993"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char vocabulary: \t\n",
            " !\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyz{|}~Â Â¡Â¢Â£Â¥Â§Â¬Â­Â®Â¯Â°Â±Â²Â³Â´ÂµÂ·ÂºÂ»Â¼Â½Ã€ÃÃ„Ã…Ã†Ã‡Ã‰ÃŽÃ‘Ã“Ã•Ã–Ã—Ã˜ÃœÃžÃŸÃ Ã¡Ã¢Ã£Ã¤Ã¥Ã¦Ã§Ã¨Ã©ÃªÃ«Ã¬Ã­Ã®Ã¯Ã°Ã±Ã²Ã³Ã´ÃµÃ¶Ã¸Ã¹ÃºÃ»Ã¼Ã½Ã¾Ä€ÄÄƒÄ…Ä‡Ä‹ÄŒÄÄÄ‘Ä“Ä™Ä›ÄÄŸÄ¡Ä§Ä©ÄªÄ«Ä°Ä±Ä¾ÅÅ‚Å„Å†ÅˆÅ‹ÅŒÅÅ‘Å’Å“Å™ÅšÅ›ÅžÅŸÅ Å¡Å£Å©Å«ÅµÅ·ÅºÅ»Å¼Å½Å¾Æ’Æ¡Æ°Æ¿Ç‚ÇŽÇÇ’Ç”Ç£Ç¥Ç§Ç«ÇµÇ·È˜È™ÈšÈ›ÈŸÈ³ÉÉ‘É’É”É•É™É›ÉœÉ¡É£É¤É¦ÉªÉ¬É¯É²É´É¾ÊÊ‚ÊƒÊŠÊŒÊŽÊÊÊ‘Ê’Ê”Ê•Ê°Ê·Ê»Ê¼Ê¾Ê¿ËˆËŒËË¤Ì€ÌÌƒÌ„ÌŠÌŒÌšÌ©ÌªÌ¯Ì°Ì²Ì¸Í¡Í¤Í­Î†ÎÎ‘Î“Î”Î•Î˜Î™ÎšÎ›ÎœÎŸÎ Î£Î¤Î¦Î§Î©Î¬Î­Î®Î¯Î±Î²Î³Î´ÎµÎ¶Î·Î¸Î¹ÎºÎ»Î¼Î½Î¾Î¿Ï€ÏÏ‚ÏƒÏ„Ï…Ï†Ï‡ÏˆÏ‰ÏŠÏŒÏÏŽÏ•ÏµÐÐ‘Ð“Ð”ÐšÐÐžÐŸÐ Ð¡Ð¢Ð£Ð¤Ð¥Ð°Ð±Ð²Ð³Ð´ÐµÐ·Ð¸Ð¹ÐºÐ»Ð¼Ð½Ð¾Ð¿Ñ€ÑÑ‚ÑƒÑ„Ñ…Ñ†Ñ‡ÑŠÑŒÑÑ—Ñ˜Ò³Ö£Ö°Ö²Ö´ÖµÖ¶Ö·Ö¸Ö¹Ö»Ö¼××‚××‘×’×“×”×•×—×˜×™×š×›×œ××ž×Ÿ× ×¡×¢×¤×¦×§×¨×©×ª×³ØŒØ¡Ø¢Ø£Ø¥Ø¦Ø§Ø¨Ø©ØªØ«Ø¬Ø­Ø®Ø¯Ø°Ø±Ø²Ø³Ø´ØµØ¶Ø·Ø¸Ø¹ØºÙÙ‚ÙƒÙ„Ù…Ù†Ù‡ÙˆÙ‰ÙŠÙŽÙÙÙ‘Ù’Ù°Ù±Ú†Ú©ÛŒà¤šà¤¤à¤¦à¤¨à¤¬à¤®à¤°à¤µà¤¶à¤¹à¤¾à¥€à¥‡à¥à¦‚à¦•à¦—à¦œà¦žà¦ à¦¤à¦¥à¦¦à¦¨à¦¬à¦°à¦²à¦¾à¦¿à§€à§à§à´‚à´®à´¯à´²à´³à´¾à¸à¸‚à¸ƒà¸„à¸…à¸‡à¸ˆà¸‰à¸à¸“à¸”à¸•à¸–à¸—à¸˜à¸™à¸šà¸›à¸œà¸à¸žà¸Ÿà¸ à¸¡à¸¢à¸£à¸¥à¸§à¸©à¸ªà¸«à¸­à¸®à¸°à¸±à¸²à¸³à¸´à¸µà¸·à¸¸à¸¹à¹€à¹à¹‚à¹ƒà¹„à¹†à¹‡à¹ˆà¹‰á€€á€„á€…á€Šá€á€‘á€’á€”á€•á€™á€šá€žá€¬á€­á€®á€¯á€±á€¶á€¸á€¹á€ºá€¼áš¼á›á›’á¸‡á¸á¸á¸—á¸¤á¸¥á¸±á¸³á¹á¹ƒá¹…á¹†á¹‡á¹“á¹›á¹Ÿá¹£á¹­á¹¯á¹µáºŠáº‹áº“áºšáº¡áº£áº¥áº§áº©áº­áº¯áº¿á»á»ƒá»…á»‡á»‹á»á»‘á»“á»•á»™á»›á»á»Ÿá»¦á»§á»©á»¯á»³á¼€á¼á¼„á¼ˆá¼á¼‘á¼”á¼•á¼™á¼ á¼¡á¼¦á¼°á¼´á¼¶á¼·á¼¸á½á½…á½á½‘á½–á½¤á½°á½²á½´á½¶á½¸á¾±á¿†á¿á¿–á¿¡á¿¦á¿¶â€‰â€Šâ€‹â€Œâ€Žâ€â€“â€”â€•â€–â€˜â€™â€œâ€â€ â€¡â€¢â€¦â€¯â€°â€²â€³â€¿â„â â¡â‚¬â‚¹â„â„“â„â„¢â„µâ„¶â†â†’â†”â†¦â‡’âˆ€âˆ‚âˆƒâˆ…âˆ†âˆ‡âˆˆâˆ‰âˆ‹âˆ‘âˆ’âˆ—âˆ˜âˆ™âˆšâˆâˆžâˆ£âˆ¥âˆ§âˆ¨âˆ«âˆ´âˆ¼â‰ˆâ‰ â‰¡â‰¤â‰¥â‰ªâ‰«âŠ‚âŠ†âŠšâŠ¢âŠ¨â‹ƒâ‹…â‹†â‹¯âŒˆâŒ‰âŒ˜â–¼â˜…â™­â™®â™¯âœ“âŸ¨âŸ©ã‹ã˜ã‚“ã‚¢ã‚¤ã‚¬ã‚¶ã‚¹ã‚ºãƒ™ãƒœãƒžãƒ ãƒ¦ãƒ©ãƒªãƒ«ãƒ³ãƒ¼ã…‡ä¸€ä¸‹ä¸šä¸­ä¸»ä¹‰ä¹‹ä¹äº§äººä»ä»£ä¼ä¼™ä¼šä½†ä½“ä½›ä¿ƒä¿—å„’å…«å…¬å…±å…´å†œåˆ©åŽåˆåå¯å‘½å’Œå˜‰å›½åœ‹åœ³å¢¨å£«å¤©å¤«å«»å­å­—å­å­¦å­¸å®—å®šå®¶å¯†å¯Œå¯¦å¯¶å°å°å·¦å¸‚å¸å¹³åº·å»ºå½“å½¬å¾·å¿ æˆæˆ°æŠ“æŠ•æ•™æ–‡æ–°æ—¥æ˜¥æ™‚æœƒæœŸæœ¬æžœæ¡ƒæ¬Šæ­£æ­¦æ°‘æ±‡æ³•æ´¥æ·±æºæ¼¢çˆ²çŽ‹çŽ°ç¾ç†ç”Ÿç”¢ç”±ç•¶ç™¾çš‡ç›¸çœ ç¿çŸ¿ç¤¾ç¥žç¦®ç§‹ç®€ç®¡ç´„çµ„ç¶“ç¹ç½‘ç¾©ç¿¼èƒ½è‡ªè‡´èŠ’èŠ±èŠ³èŽŠè¯èžè§†è¨“è©èªžè«§è«¸è°è´¾èµ„è·ƒé‹é“é¾é‘«é“¶é™°é™½é©éŸ³éŸ»é¢‘é«”êµ­ë¦¬ì•„ì–´ì—­ì¹´í”„í•œï¬ïºŸï»¿ï¼ˆï¼‰ï¼Œð¤ð¤‹ð¤ð“‚‹ð“ˆ‰ð“ˆ–ð“ \n",
            "vocabulary size: 953\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 802 chars! \n",
        "# I know we should do data cleaning, unwanted char/word removal, etc etc...\n",
        "# but let's just go with it and see what happens..."
      ],
      "metadata": {
        "id": "1w5g0R2KprSy"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a simple char to int tokenization strategy (encoding & decoding)\n",
        "\n",
        "# mapping between chars and ints vased on data vocab\n",
        "c2i = { ch:i for i,ch in enumerate(char_vocab) }\n",
        "i2c = { i:ch for i,ch in enumerate(char_vocab) }\n",
        "\n",
        "# encode: text string --> 1D num vector\n",
        "encode = lambda txt_str: [c2i[ch] for ch in txt_str]\n",
        "\n",
        "# decode: encoded 1D num vector --> text string\n",
        "decode = lambda num_vect: ''.join([i2c[i] for i in num_vect])"
      ],
      "metadata": {
        "id": "zft3QEX0lFVz"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's test our encoder and decoder\n",
        "print(encode(\"hello LLM o/\"))\n",
        "print(decode([74, 71, 78, 78, 81, 2, 46, 46, 47, 2, 81, 17]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8rqOtBsm4T8",
        "outputId": "f66cb590-466e-44f7-b7b5-8f82938b5903"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[74, 71, 78, 78, 81, 2, 46, 46, 47, 2, 81, 17]\n",
            "hello LLM o/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's see how it looks with first 100 chars from our raw text data\n",
        "print(text_data[:100])\n",
        "print(encode(text_data[:100]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FujrZtLypfVS",
        "outputId": "3195aaa5-3cdc-4e01-a371-17b4ecb4c359"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This page allows users to search multiple sources for a book given a 10- or 13-digit International S\n",
            "[54, 74, 75, 85, 2, 82, 67, 73, 71, 2, 67, 78, 78, 81, 89, 85, 2, 87, 85, 71, 84, 85, 2, 86, 81, 2, 85, 71, 67, 84, 69, 74, 2, 79, 87, 78, 86, 75, 82, 78, 71, 2, 85, 81, 87, 84, 69, 71, 85, 2, 72, 81, 84, 2, 67, 2, 68, 81, 81, 77, 2, 73, 75, 88, 71, 80, 2, 67, 2, 19, 18, 15, 2, 81, 84, 2, 19, 21, 15, 70, 75, 73, 75, 86, 2, 43, 80, 86, 71, 84, 80, 67, 86, 75, 81, 80, 67, 78, 2, 53]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# so our basic encoding works!"
      ],
      "metadata": {
        "id": "T1jOGvz4pmDr"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tt4oYmCF41SJ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using the Encoder/Tokenizer for Input Embedding"
      ],
      "metadata": {
        "id": "nrsvq94w64-t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let's now encode the entire text dataset and store it into torch.Tensor\n",
        "# (blindly following Karpathy here), using PyTorch (https://pytorch.org)\n",
        "# could have used TF, or other techniques, but will see in future\n",
        "\n",
        "import torch"
      ],
      "metadata": {
        "id": "IOFqQ1rYq6ZP"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use our encoder and wrap it into PyTorch tensors\n",
        "data = torch.tensor(encode(text_data), dtype=torch.long)\n",
        "\n",
        "# let's take a look at shape and dtype\n",
        "print(data.shape, data.dtype)\n",
        "\n",
        "# and the first 100 chars from earlier\n",
        "print(data[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8GG_03XHq6ch",
        "outputId": "1ee84ff3-a4b4-49f4-9632-18514b502757"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([15795757]) torch.int64\n",
            "tensor([54, 74, 75, 85,  2, 82, 67, 73, 71,  2, 67, 78, 78, 81, 89, 85,  2, 87,\n",
            "        85, 71, 84, 85,  2, 86, 81,  2, 85, 71, 67, 84, 69, 74,  2, 79, 87, 78,\n",
            "        86, 75, 82, 78, 71,  2, 85, 81, 87, 84, 69, 71, 85,  2, 72, 81, 84,  2,\n",
            "        67,  2, 68, 81, 81, 77,  2, 73, 75, 88, 71, 80,  2, 67,  2, 19, 18, 15,\n",
            "         2, 81, 84,  2, 19, 21, 15, 70, 75, 73, 75, 86,  2, 43, 80, 86, 71, 84,\n",
            "        80, 67, 86, 75, 81, 80, 67, 78,  2, 53])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ef-kZxid5Mbo"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cFn4kLiT5MeF"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-qxITsIe5Mm2"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References"
      ],
      "metadata": {
        "id": "gj9uXSk77LBU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reading Materials and References for future:\n",
        "- https://towardsdatascience.com/word-embeddings-with-code2vec-glove-and-spacy-5b26420bf632\n",
        "- https://huggingface.co/course/chapter6/8\n",
        "- https://neptune.ai/blog/tokenization-in-nlp\n",
        "- https://research.aimultiple.com/large-language-model-training/\n",
        "- https://towardsdatascience.com/word-embeddings-in-2020-review-with-code-examples-11eb39a1ee6d\n"
      ],
      "metadata": {
        "id": "mxaoRwUfq61w"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1hSdp2MurEa2"
      },
      "execution_count": 11,
      "outputs": []
    }
  ]
}