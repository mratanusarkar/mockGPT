{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNqyb6ibfS3i7pZV239cvrc"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import and Read the data"
      ],
      "metadata": {
        "id": "T_-GaHlNi3sZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kPDjZ9zhmM1e"
      },
      "outputs": [],
      "source": [
        "# read the input data file\n",
        "with open('raw-text-data.txt', 'r', encoding='utf-8') as f:\n",
        "    text_data = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# length of the data in chars\n",
        "print(\"length of the data in chars:\", len(text_data))\n",
        "\n",
        "# let's look at the first 100 characters\n",
        "print(text_data[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKyNzeQ5jlii",
        "outputId": "1fa29ee0-d9a8-435e-9d8d-93d844c687bc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of the data in chars: 15795757\n",
            "This page allows users to search multiple sources for a book given a 10- or 13-digit International S\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# randomize the text contents"
      ],
      "metadata": {
        "id": "fJqUg_iuAoH7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let's look at how many line-breaks are there in the data\n",
        "len(text_data.splitlines())"
      ],
      "metadata": {
        "id": "EmF3QQUX1kwK",
        "outputId": "5ebff84f-7813-4f7c-ee51-e3d0eade7ad6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2000"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# so from analysis, we see the data source contained full scraped data into one single line\n",
        "# since we had 1000 source files, seperated by '\\n\\n', hence 2000 line splits\n",
        "# we should mix up the content, else the model will only learn content from data sources\n",
        "# that comes in the beginning and falls into train dataset"
      ],
      "metadata": {
        "id": "6ObsVIuTABHZ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's try looking at end of sentenses using full-stop punctuation mark\n",
        "len(text_data.split('. '))"
      ],
      "metadata": {
        "id": "ZqtEssEUABKp",
        "outputId": "80d3e4c7-725c-4358-a818-c56d7436038d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "68059"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# looks good! let's split it this way, and randomize!\n",
        "rand_text_data = []\n",
        "for content in text_data.splitlines():\n",
        "    if content:\n",
        "        sentence_list = content.split('. ')\n",
        "        stripped_list = [sentence.strip() for sentence in sentence_list]\n",
        "        rand_text_data.extend(stripped_list)"
      ],
      "metadata": {
        "id": "cWgmt9XPABNz"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"length: \", len(rand_text_data))\n",
        "rand_text_data[8:12]"
      ],
      "metadata": {
        "id": "CZ_GhyQoIyKt",
        "outputId": "f57c2417-375a-4867-cadc-ff5df46e647d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length:  69058\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Google books often lists other editions of a book and related books under the \"about this book\" link',\n",
              " 'You can convert between 10 and 13 digit ISBNs with these tools: Get free access to research! Research tools and services Outreach Get involved',\n",
              " \"Successful military and police takeover \\xa0Royal Thai Armed Forces Royal Thai Police The 1991 Thai coup d'Ã©tat was a military coup against the democratic Chatichai Choonhavan government, carried out by Thai military leaders on 23 February\",\n",
              " 'Although the figure head was Sunthorn Kongsompong, there was a military influence from military leaders, Chavalit Yongchaiyudh, Suchinda Kraprayoon, and Kaset Rojananil in the conflict']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's try to randomly shuffle the elements of a list \n",
        "import random\n",
        "random.seed(2023)\n",
        "\n",
        "mylist = list(range(20))\n",
        "print(mylist)\n",
        "\n",
        "random.shuffle(mylist)  # seed works!\n",
        "print(mylist)"
      ],
      "metadata": {
        "id": "gT_eyuvED-IS",
        "outputId": "953a95aa-ad3a-4c81-b49f-ee3a43d6f977",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
            "[2, 7, 3, 0, 5, 6, 11, 9, 8, 4, 18, 13, 17, 1, 15, 16, 10, 19, 14, 12]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's randomly shuffle the lines in rand_text_data\n",
        "random.seed(2023)\n",
        "random.shuffle(rand_text_data)\n",
        "\n",
        "# let's check with the same piece of code above for rand_text_data\n",
        "print(\"length: \", len(rand_text_data))\n",
        "rand_text_data[8:12]"
      ],
      "metadata": {
        "id": "0lP0GG9ZOd_S",
        "outputId": "e359213a-d932-4a39-a459-b1b476b526fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length:  69058\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A chord may be built upon any note of a musical scale',\n",
              " 'While the first derivative test identifies points that might be extrema, this test does not distinguish a point that is a minimum from one that is a maximum or one that is neither',\n",
              " '\"Playfair, John\"',\n",
              " 'The commission supported increasing enforcement against undocumented migrants and their employers, eliminating visa preferences for siblings and adult children of U.S']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's form our dataset by joining the lines together\n",
        "# since the sentences would be disjointed and out of context from each other, \n",
        "# it's better to join with a '\\n'\n",
        "text_data = '. \\n'.join(rand_text_data)"
      ],
      "metadata": {
        "id": "spxTHARZQKXC"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (yes, we would loose context between sentences, but now the data won't be biased)\n",
        "# we can decide to choose any of the techniques later\n",
        "\n",
        "# length of the data in chars\n",
        "print(\"length of the data in chars:\", len(text_data))\n",
        "\n",
        "# let's look at the first 100 characters\n",
        "print(text_data[:100])"
      ],
      "metadata": {
        "id": "aT1PjgCmGNpP",
        "outputId": "20092ea0-f481-4f8a-dd75-42a20632a730",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of the data in chars: 15861052\n",
            "The highly miniaturized product, about the size of a cigarette lighter and with a 4.6-inch long USB \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3NIWmZUqYpxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoders and Tokenizers"
      ],
      "metadata": {
        "id": "kmKm2TQG6w4V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "some popular tokenizers to look out for:\n",
        "- [sentencepiece](https://github.com/google/sentencepiece) (google)\n",
        "- [tiktoken](https://github.com/openai/tiktoken) (openai)\n",
        "- [word2vec](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/word2vec.ipynb) (google tf)\n",
        "- [code2vec](https://github.com/tech-srl/code2vec) (code2vec.org)\n",
        "- [GloVe](https://nlp.stanford.edu/projects/glove/) (nlp stanford)\n",
        "- [spaCy](https://spacy.io/usage/linguistic-features#vectors-similarity) (explosion.ai)\n",
        "\n",
        "we can look up hugging face, [chapter 6](https://huggingface.co/course/chapter6/1) for more tokenizers and [tokenizer_summary](https://huggingface.co/docs/transformers/tokenizer_summary) from their transformers page. some of the useful encoders are as below:\n",
        "- Byte Pair Encoding (BPE)\n",
        "- WordPiece tokenization\n",
        "- Unigram tokenization\n"
      ],
      "metadata": {
        "id": "GQD991eSqF5V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "for tokenizing and embedding code (say for example, C++), we can use:\n",
        "- [Subword tokenizers](https://www.tensorflow.org/text/guide/subwords_tokenizer)\n",
        "- Syntax-aware Tokenization\n",
        "- Code-specific Tokenization\n",
        "- Clang\n",
        "- TreeSitter\n",
        "- CodeBERT\n",
        "- Code2Vec\n",
        "- Graph-based Neural Networks"
      ],
      "metadata": {
        "id": "UEfHnQ_L06f7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let's build a basic, simple char embedding to int to start with."
      ],
      "metadata": {
        "id": "lgQGNRB21nQR"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's look at the unique chars in the text data\n",
        "char_vocab = sorted(list(set(text_data)))\n",
        "vocab_size = len(char_vocab)\n",
        "print(\"char vocabulary:\", ''.join(char_vocab))\n",
        "print(\"vocabulary size:\", vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lvED03p-jmQV",
        "outputId": "7526ea37-430f-42a0-fee3-da3be089950b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char vocabulary: \t\n",
            " !\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyz{|}~Â Â¡Â¢Â£Â¥Â§Â¬Â­Â®Â¯Â°Â±Â²Â³Â´ÂµÂ·ÂºÂ»Â¼Â½Ã€ÃÃ„Ã…Ã†Ã‡Ã‰ÃŽÃ‘Ã“Ã•Ã–Ã—Ã˜ÃœÃžÃŸÃ Ã¡Ã¢Ã£Ã¤Ã¥Ã¦Ã§Ã¨Ã©ÃªÃ«Ã¬Ã­Ã®Ã¯Ã°Ã±Ã²Ã³Ã´ÃµÃ¶Ã¸Ã¹ÃºÃ»Ã¼Ã½Ã¾Ä€ÄÄƒÄ…Ä‡Ä‹ÄŒÄÄÄ‘Ä“Ä™Ä›ÄÄŸÄ¡Ä§Ä©ÄªÄ«Ä°Ä±Ä¾ÅÅ‚Å„Å†ÅˆÅ‹ÅŒÅÅ‘Å’Å“Å™ÅšÅ›ÅžÅŸÅ Å¡Å£Å©Å«ÅµÅ·ÅºÅ»Å¼Å½Å¾Æ’Æ¡Æ°Æ¿Ç‚ÇŽÇÇ’Ç”Ç£Ç¥Ç§Ç«ÇµÇ·È˜È™ÈšÈ›ÈŸÈ³ÉÉ‘É’É”É•É™É›ÉœÉ¡É£É¤É¦ÉªÉ¬É¯É²É´É¾ÊÊ‚ÊƒÊŠÊŒÊŽÊÊÊ‘Ê’Ê”Ê•Ê°Ê·Ê»Ê¼Ê¾Ê¿ËˆËŒËË¤Ì€ÌÌƒÌ„ÌŠÌŒÌšÌ©ÌªÌ¯Ì°Ì²Ì¸Í¡Í¤Í­Î†ÎÎ‘Î“Î”Î•Î˜Î™ÎšÎ›ÎœÎŸÎ Î£Î¤Î¦Î§Î©Î¬Î­Î®Î¯Î±Î²Î³Î´ÎµÎ¶Î·Î¸Î¹ÎºÎ»Î¼Î½Î¾Î¿Ï€ÏÏ‚ÏƒÏ„Ï…Ï†Ï‡ÏˆÏ‰ÏŠÏŒÏÏŽÏ•ÏµÐÐ‘Ð“Ð”ÐšÐÐžÐŸÐ Ð¡Ð¢Ð£Ð¤Ð¥Ð°Ð±Ð²Ð³Ð´ÐµÐ·Ð¸Ð¹ÐºÐ»Ð¼Ð½Ð¾Ð¿Ñ€ÑÑ‚ÑƒÑ„Ñ…Ñ†Ñ‡ÑŠÑŒÑÑ—Ñ˜Ò³Ö£Ö°Ö²Ö´ÖµÖ¶Ö·Ö¸Ö¹Ö»Ö¼××‚××‘×’×“×”×•×—×˜×™×š×›×œ××ž×Ÿ× ×¡×¢×¤×¦×§×¨×©×ª×³ØŒØ¡Ø¢Ø£Ø¥Ø¦Ø§Ø¨Ø©ØªØ«Ø¬Ø­Ø®Ø¯Ø°Ø±Ø²Ø³Ø´ØµØ¶Ø·Ø¸Ø¹ØºÙÙ‚ÙƒÙ„Ù…Ù†Ù‡ÙˆÙ‰ÙŠÙŽÙÙÙ‘Ù’Ù°Ù±Ú†Ú©ÛŒà¤šà¤¤à¤¦à¤¨à¤¬à¤®à¤°à¤µà¤¶à¤¹à¤¾à¥€à¥‡à¥à¦‚à¦•à¦—à¦œà¦žà¦ à¦¤à¦¥à¦¦à¦¨à¦¬à¦°à¦²à¦¾à¦¿à§€à§à§à´‚à´®à´¯à´²à´³à´¾à¸à¸‚à¸ƒà¸„à¸…à¸‡à¸ˆà¸‰à¸à¸“à¸”à¸•à¸–à¸—à¸˜à¸™à¸šà¸›à¸œà¸à¸žà¸Ÿà¸ à¸¡à¸¢à¸£à¸¥à¸§à¸©à¸ªà¸«à¸­à¸®à¸°à¸±à¸²à¸³à¸´à¸µà¸·à¸¸à¸¹à¹€à¹à¹‚à¹ƒà¹„à¹†à¹‡à¹ˆà¹‰á€€á€„á€…á€Šá€á€‘á€’á€”á€•á€™á€šá€žá€¬á€­á€®á€¯á€±á€¶á€¸á€¹á€ºá€¼áš¼á›á›’á¸‡á¸á¸á¸—á¸¤á¸¥á¸±á¸³á¹á¹ƒá¹…á¹†á¹‡á¹“á¹›á¹Ÿá¹£á¹­á¹¯á¹µáºŠáº‹áº“áºšáº¡áº£áº¥áº§áº©áº­áº¯áº¿á»á»ƒá»…á»‡á»‹á»á»‘á»“á»•á»™á»›á»á»Ÿá»¦á»§á»©á»¯á»³á¼€á¼á¼„á¼ˆá¼á¼‘á¼”á¼•á¼™á¼ á¼¡á¼¦á¼°á¼´á¼¶á¼·á¼¸á½á½…á½á½‘á½–á½¤á½°á½²á½´á½¶á½¸á¾±á¿†á¿á¿–á¿¡á¿¦á¿¶â€‰â€Šâ€‹â€Œâ€Žâ€â€“â€”â€•â€–â€˜â€™â€œâ€â€ â€¡â€¢â€¦â€¯â€°â€²â€³â€¿â„â â¡â‚¬â‚¹â„â„“â„â„¢â„µâ„¶â†â†’â†”â†¦â‡’âˆ€âˆ‚âˆƒâˆ…âˆ†âˆ‡âˆˆâˆ‰âˆ‹âˆ‘âˆ’âˆ—âˆ˜âˆ™âˆšâˆâˆžâˆ£âˆ¥âˆ§âˆ¨âˆ«âˆ´âˆ¼â‰ˆâ‰ â‰¡â‰¤â‰¥â‰ªâ‰«âŠ‚âŠ†âŠšâŠ¢âŠ¨â‹ƒâ‹…â‹†â‹¯âŒˆâŒ‰âŒ˜â–¼â˜…â™­â™®â™¯âœ“âŸ¨âŸ©ã‹ã˜ã‚“ã‚¢ã‚¤ã‚¬ã‚¶ã‚¹ã‚ºãƒ™ãƒœãƒžãƒ ãƒ¦ãƒ©ãƒªãƒ«ãƒ³ãƒ¼ã…‡ä¸€ä¸‹ä¸šä¸­ä¸»ä¹‰ä¹‹ä¹äº§äººä»ä»£ä¼ä¼™ä¼šä½†ä½“ä½›ä¿ƒä¿—å„’å…«å…¬å…±å…´å†œåˆ©åŽåˆåå¯å‘½å’Œå˜‰å›½åœ‹åœ³å¢¨å£«å¤©å¤«å«»å­å­—å­å­¦å­¸å®—å®šå®¶å¯†å¯Œå¯¦å¯¶å°å°å·¦å¸‚å¸å¹³åº·å»ºå½“å½¬å¾·å¿ æˆæˆ°æŠ“æŠ•æ•™æ–‡æ–°æ—¥æ˜¥æ™‚æœƒæœŸæœ¬æžœæ¡ƒæ¬Šæ­£æ­¦æ°‘æ±‡æ³•æ´¥æ·±æºæ¼¢çˆ²çŽ‹çŽ°ç¾ç†ç”Ÿç”¢ç”±ç•¶ç™¾çš‡ç›¸çœ ç¿çŸ¿ç¤¾ç¥žç¦®ç§‹ç®€ç®¡ç´„çµ„ç¶“ç¹ç½‘ç¾©ç¿¼èƒ½è‡ªè‡´èŠ’èŠ±èŠ³èŽŠè¯èžè§†è¨“è©èªžè«§è«¸è°è´¾èµ„è·ƒé‹é“é¾é‘«é“¶é™°é™½é©éŸ³éŸ»é¢‘é«”êµ­ë¦¬ì•„ì–´ì—­ì¹´í”„í•œï¬ïºŸï»¿ï¼ˆï¼‰ï¼Œð¤ð¤‹ð¤ð“‚‹ð“ˆ‰ð“ˆ–ð“ \n",
            "vocabulary size: 953\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 802 chars! \n",
        "# I know we should do data cleaning, unwanted char/word removal, etc etc...\n",
        "# but let's just go with it and see what happens..."
      ],
      "metadata": {
        "id": "1w5g0R2KprSy"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a simple char to int tokenization strategy (encoding & decoding)\n",
        "\n",
        "# mapping between chars and ints vased on data vocab\n",
        "c2i = { ch:i for i,ch in enumerate(char_vocab) }\n",
        "i2c = { i:ch for i,ch in enumerate(char_vocab) }\n",
        "\n",
        "# encode: text string --> 1D num vector\n",
        "encode = lambda txt_str: [c2i[ch] for ch in txt_str]\n",
        "\n",
        "# decode: encoded 1D num vector --> text string\n",
        "decode = lambda num_vect: ''.join([i2c[i] for i in num_vect])"
      ],
      "metadata": {
        "id": "zft3QEX0lFVz"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's test our encoder and decoder\n",
        "print(encode(\"hello LLM o/\"))\n",
        "print(decode([74, 71, 78, 78, 81, 2, 46, 46, 47, 2, 81, 17]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8rqOtBsm4T8",
        "outputId": "3e8865fa-40e2-44c3-a5ae-f4dfb3a25259"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[74, 71, 78, 78, 81, 2, 46, 46, 47, 2, 81, 17]\n",
            "hello LLM o/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's see how it looks with first 100 chars from our raw text data\n",
        "print(text_data[:100])\n",
        "print(encode(text_data[:100]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FujrZtLypfVS",
        "outputId": "bf652c0e-8595-4f02-e238-20682b6ecbac"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The highly miniaturized product, about the size of a cigarette lighter and with a 4.6-inch long USB \n",
            "[54, 74, 71, 2, 74, 75, 73, 74, 78, 91, 2, 79, 75, 80, 75, 67, 86, 87, 84, 75, 92, 71, 70, 2, 82, 84, 81, 70, 87, 69, 86, 14, 2, 67, 68, 81, 87, 86, 2, 86, 74, 71, 2, 85, 75, 92, 71, 2, 81, 72, 2, 67, 2, 69, 75, 73, 67, 84, 71, 86, 86, 71, 2, 78, 75, 73, 74, 86, 71, 84, 2, 67, 80, 70, 2, 89, 75, 86, 74, 2, 67, 2, 22, 16, 24, 15, 75, 80, 69, 74, 2, 78, 81, 80, 73, 2, 55, 53, 36, 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# so our basic encoding works!"
      ],
      "metadata": {
        "id": "T1jOGvz4pmDr"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tt4oYmCF41SJ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using the Encoder/Tokenizer for Input Embedding"
      ],
      "metadata": {
        "id": "nrsvq94w64-t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let's now encode the entire text dataset and store it into torch.Tensor\n",
        "# (blindly following Karpathy here), using PyTorch (https://pytorch.org)\n",
        "# could have used TF, or other techniques, but will see in future\n",
        "\n",
        "import torch"
      ],
      "metadata": {
        "id": "IOFqQ1rYq6ZP"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use our encoder and wrap it into PyTorch tensors\n",
        "data = torch.tensor(encode(text_data), dtype=torch.long)\n",
        "\n",
        "# let's take a look at shape and dtype\n",
        "print(data.shape, data.dtype)\n",
        "\n",
        "# and the first 100 chars from earlier\n",
        "print(data[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8GG_03XHq6ch",
        "outputId": "c3419a0f-f28c-48c6-825c-4937d7718299"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([15861052]) torch.int64\n",
            "tensor([54, 74, 71,  2, 74, 75, 73, 74, 78, 91,  2, 79, 75, 80, 75, 67, 86, 87,\n",
            "        84, 75, 92, 71, 70,  2, 82, 84, 81, 70, 87, 69, 86, 14,  2, 67, 68, 81,\n",
            "        87, 86,  2, 86, 74, 71,  2, 85, 75, 92, 71,  2, 81, 72,  2, 67,  2, 69,\n",
            "        75, 73, 67, 84, 71, 86, 86, 71,  2, 78, 75, 73, 74, 86, 71, 84,  2, 67,\n",
            "        80, 70,  2, 89, 75, 86, 74,  2, 67,  2, 22, 16, 24, 15, 75, 80, 69, 74,\n",
            "         2, 78, 81, 80, 73,  2, 55, 53, 36,  2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ef-kZxid5Mbo"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cFn4kLiT5MeF"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-qxITsIe5Mm2"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References"
      ],
      "metadata": {
        "id": "gj9uXSk77LBU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reading Materials and References for future:\n",
        "- https://towardsdatascience.com/word-embeddings-with-code2vec-glove-and-spacy-5b26420bf632\n",
        "- https://huggingface.co/course/chapter6/8\n",
        "- https://neptune.ai/blog/tokenization-in-nlp\n",
        "- https://research.aimultiple.com/large-language-model-training/\n",
        "- https://towardsdatascience.com/word-embeddings-in-2020-review-with-code-examples-11eb39a1ee6d\n"
      ],
      "metadata": {
        "id": "mxaoRwUfq61w"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1hSdp2MurEa2"
      },
      "execution_count": 20,
      "outputs": []
    }
  ]
}